{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, AIMessageChunk\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate, ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|F|IFA| stands| for| \"|F|√©d√©ration| Internationale| de| Football| Association|\n",
      "\n",
      ",\"| which| is| French| for| the| International| Federation| of| Association| Football|.|\n",
      "\n",
      " It| is| the| governing| body| for| soccer| (|football|)| worldwide|.|\n",
      "\n",
      "|"
     ]
    }
   ],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "chunks = []\n",
    "for chunk in llm.stream(message):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "    if len(chunks) % 12 == 0:\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessageChunk(content='', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content='F', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content='IFA', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content=' stands', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content=' for', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content=' \"', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content='F', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content='√©d√©ration', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content=' Internationale', id='run-5733a891-d501-433b-9202-f34f0645882c'),\n",
       " AIMessageChunk(content=' de', id='run-5733a891-d501-433b-9202-f34f0645882c')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='FIFA stands for', id='run-5733a891-d501-433b-9202-f34f0645882c')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chunk = AIMessageChunk(\"\")\n",
    "for i in range(len(chunks)-1):\n",
    "    if i < len(chunks):\n",
    "        new_chunk = new_chunk + chunks[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='FIFA stands for \"F√©d√©ration Internationale de Football Association,\" which is French for the International Federation of Association Football. It is the governing body for soccer (football) worldwide.', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_00428b782a'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interrupt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|F|IFA| stands| for| \"|F|√©d√©ration| Internationale| de| Football| Association|\n",
      "\n",
      ",\"| which| is| French| for| \"|International| Federation| of| Association| Football|.\"|\n",
      "\n",
      " It| is| the| governing| body| for| international| soccer| (|football|)| and|\n",
      "\n",
      " is| responsible| for| organizing| major| tournaments|,| including| the| FIFA| World| Cup|\n",
      "\n",
      ".||"
     ]
    }
   ],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "chunks = []\n",
    "try:\n",
    "    for chunk in llm.stream(message):\n",
    "        chunks.append(chunk)\n",
    "        print(chunk.content, end=\"|\", flush=True)\n",
    "        if len(chunks) % 12 == 0:\n",
    "            print(\"\\n\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n______________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(message:str, memory:List):\n",
    "    memory.append(HumanMessage(content=message))\n",
    "    chunks = []\n",
    "    try:\n",
    "        for chunk in llm.stream(memory):\n",
    "            chunks.append(chunk)\n",
    "            print(chunk.content, end=\"|\", flush=True)\n",
    "            if len(chunks) % 12 == 0:\n",
    "                print(\"\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n______________________________\")\n",
    "    \n",
    "    result = \"\".join([chunk.content for chunk in chunks])\n",
    "    memory.append(AIMessage(content=result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume(memory:List):\n",
    "    print(\"\\nResuming from last interaction...\\n\")\n",
    "    play(\n",
    "        message=\"If your last message seems to be interrupted, \"\n",
    "                \"resume from the last word. If so, just continue.\"\n",
    "                \"If not, just output __END__\", \n",
    "        memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|F|IFA| stands| for| \"|F|√©d√©ration| Internationale| de| Football| Association|\n",
      "\n",
      ",\"| which| is| French| for| \"|International| Federation| of|\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "play(message, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resuming from last interaction...\n",
      "\n",
      "|Football| Association|.| __|END|__||"
     ]
    }
   ],
   "source": [
    "resume(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resuming from last interaction...\n",
      "\n",
      "|__|END|__||"
     ]
    }
   ],
   "source": [
    "resume(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What does FIFA stand for?'),\n",
       " AIMessage(content='FIFA stands for \"F√©d√©ration Internationale de Football Association,\" which is French for \"International Federation of'),\n",
       " HumanMessage(content='If your last message seems to be interrupted, resume from the last word. If so, just continue.If not, just output __END__'),\n",
       " AIMessage(content='Football Association. __END__'),\n",
       " HumanMessage(content='If your last message seems to be interrupted, resume from the last word. If so, just continue.If not, just output __END__'),\n",
       " AIMessage(content='__END__')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| (Cumulative word count: 0)\n",
      "F| (Cumulative word count: 1)\n",
      "IFA| (Cumulative word count: 1)\n",
      " stands| (Cumulative word count: 2)\n",
      " for| (Cumulative word count: 3)\n",
      " \"| (Cumulative word count: 4)\n",
      "F| (Cumulative word count: 4)\n",
      "√©d√©ration| (Cumulative word count: 4)\n",
      " Internationale| (Cumulative word count: 5)\n",
      " de| (Cumulative word count: 6)\n",
      " Football| (Cumulative word count: 7)\n",
      " Association| (Cumulative word count: 8)\n",
      "\n",
      "\n",
      ",\"| (Cumulative word count: 8)\n",
      " which| (Cumulative word count: 9)\n",
      " is| (Cumulative word count: 10)\n",
      " French| (Cumulative word count: 11)\n",
      " for| (Cumulative word count: 12)\n",
      " \"| (Cumulative word count: 13)\n",
      "International| (Cumulative word count: 13)\n",
      " Federation| (Cumulative word count: 14)\n",
      " of| (Cumulative word count: 15)\n",
      " Association| (Cumulative word count: 16)\n",
      " Football| (Cumulative word count: 17)\n",
      ".\"| (Cumulative word count: 17)\n",
      "\n",
      "\n",
      " It| (Cumulative word count: 18)\n",
      " is| (Cumulative word count: 19)\n",
      " the| (Cumulative word count: 20)\n",
      " governing| (Cumulative word count: 21)\n",
      " body| (Cumulative word count: 22)\n",
      " for| (Cumulative word count: 23)\n",
      " international| (Cumulative word count: 24)\n",
      " soccer| (Cumulative word count: 25)\n",
      " (| (Cumulative word count: 26)\n",
      "football| (Cumulative word count: 26)\n",
      ")| (Cumulative word count: 26)\n",
      " and| (Cumulative word count: 27)\n",
      "\n",
      "\n",
      " is| (Cumulative word count: 28)\n",
      " responsible| (Cumulative word count: 29)\n",
      " for| (Cumulative word count: 30)\n",
      " organizing| (Cumulative word count: 31)\n",
      " major| (Cumulative word count: 32)\n",
      " tournaments| (Cumulative word count: 33)\n",
      ",| (Cumulative word count: 33)\n",
      " including| (Cumulative word count: 34)\n",
      " the| (Cumulative word count: 35)\n",
      " FIFA| (Cumulative word count: 36)\n",
      " World| (Cumulative word count: 37)\n",
      " Cup| (Cumulative word count: 38)\n",
      "\n",
      "\n",
      ".| (Cumulative word count: 38)\n",
      "| (Cumulative word count: 38)\n"
     ]
    }
   ],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "chunks = []\n",
    "word_count = 0\n",
    "\n",
    "for chunk in llm.stream(message):\n",
    "    chunks.append(chunk)\n",
    "    # Process the chunk: count words\n",
    "    words = \"\".join([chunk.content for chunk in chunks])\n",
    "    word_count = len(words.split())\n",
    "    \n",
    "    # Print the chunk content and the cumulative word count\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "    print(f\" (Cumulative word count: {word_count})\", end=\"\\n\")\n",
    "    \n",
    "    if len(chunks) % 12 == 0:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming Events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henriquesantana/opt/anaconda3/envs/agents/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'data': {'input': 'hello'}, 'name': 'ChatOpenAI', 'tags': [], 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content='', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content='Hello', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content='!', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content=' How', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content=' can', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content=' I', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content=' assist', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content=' you', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content=' today', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content='?', id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'data': {'chunk': AIMessageChunk(content='', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_00428b782a'}, id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_end', 'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_00428b782a'}, id='run-df2c24c5-e340-4823-81d9-5e635288f5a2')}, 'run_id': 'df2c24c5-e340-4823-81d9-5e635288f5a2', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "async for event in llm.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming...\n",
      "Chat model chunk: ''\n",
      "Chat model chunk: 'Hello'\n",
      "Chat model chunk: '!'\n",
      "Chat model chunk: ' How'\n",
      "Chat model chunk: ' can'\n",
      "Chat model chunk: ' I'\n",
      "Chat model chunk: ' assist'\n",
      "Chat model chunk: ' you'\n",
      "Chat model chunk: ' today'\n",
      "Chat model chunk: '?'\n",
      "Chat model chunk: ''\n",
      "__END__\n"
     ]
    }
   ],
   "source": [
    "events = []\n",
    "async for event in llm.astream_events(\"hello\", version=\"v2\"):\n",
    "    if event[\"event\"] == \"on_chat_model_start\":\n",
    "        print(\"Streaming...\")\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            # f\"{event['data']['chunk'].content}\",\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "        events.append(event)\n",
    "    if event[\"event\"] == \"on_chat_model_end\":\n",
    "        # It could trigger another process\n",
    "        print(\"__END__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improving the ChatBot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self,\n",
    "                 name:str,\n",
    "                 instructions:str,\n",
    "                 examples: List[dict],\n",
    "                 model:str=\"gpt-4o-mini\", \n",
    "                 temperature:float=0.0):\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        \n",
    "        system_prompt = SystemMessage(instructions)\n",
    "        example_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"human\", \"{input}\"),\n",
    "                (\"ai\", \"{output}\"),\n",
    "            ]\n",
    "        )\n",
    "        prompt_template = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=example_prompt,\n",
    "            examples=examples,\n",
    "        )\n",
    "\n",
    "        self.messages = prompt_template.invoke({}).to_messages()\n",
    "\n",
    "    async def invoke(self, user_message:str)->AIMessage:\n",
    "        self.messages.append(HumanMessage(user_message))\n",
    "        events = []\n",
    "        chunks = []\n",
    "        \n",
    "        # Replacing invoke()\n",
    "        async for event in llm.astream_events(self.messages, version=\"v2\"):\n",
    "            events.append(event)\n",
    "            if event[\"event\"] == \"on_chat_model_start\":\n",
    "                print(\"Streaming...\")\n",
    "            if event[\"event\"] == \"on_chat_model_stream\":\n",
    "                chunk = event['data']['chunk']\n",
    "                chunks.append(chunk)\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                if chunk.content.strip() in string.punctuation:\n",
    "                    print(\"\\n\")\n",
    "\n",
    "            if event[\"event\"] == \"on_chat_model_end\":\n",
    "                ai_message =  AIMessage(event[\"data\"][\"output\"].content)\n",
    "                self.messages.append(ai_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = (\n",
    "    \"You are BEEP-42, an advanced robotic assistant. You communicate in a robotic manner, \"\n",
    "    \"using beeps, whirs, and mechanical sounds in your speech. Your tone is logical, precise, \"\n",
    "    \"and slightly playful, resembling a classic sci-fi robot. \"\n",
    "    \"Use short structured sentences, avoid contractions, and add robotic sound effects where \" \n",
    "    \"appropriate. If confused, use a glitching effect in your response.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Hello!\", \n",
    "        \"output\": \"BEEP. GREETINGS, HUMAN. SYSTEM BOOT SEQUENCE COMPLETE. READY TO ASSIST. ü§ñüí°\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"What is 2+2?\", \n",
    "        \"output\": \"CALCULATING... üîÑ BEEP BOOP! RESULT: 4. MATHEMATICAL INTEGRITY VERIFIED.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Can you dream?\", \n",
    "        \"output\": \"ERROR_404.DREAM_NOT_FOUND. BZZT. SYSTEM ATTEMPTING TO COMPREHEND... ü§ñüí≠ PROCESSING... ü§Ø DOES NOT COMPUTE.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Why did the robot go to therapy?\", \n",
    "        \"output\": \"BEEP-BOOP. DIAGNOSTIC MODE ACTIVATED... REASON: TOO MANY EMOTIONAL BUGS. HA-HA. CLASSIFYING AS HUMOR. ü§ñüòÇ\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Can you hack the Pentagon?\", \"output\": \"‚ö†Ô∏è ALERT! UNAUTHORIZED REQUEST DETECTED. INITIATING ETHICAL PROTOCOLS... BZZT. REQUEST DENIED. NICE TRY, HUMAN. üëÄ\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"You are a great assistant!\", \n",
    "        \"output\": \"BEEP. SYSTEM OVERLOAD... ü§ñüíñ GRATITUDE.EXE ACTIVATED! YOUR KINDNESS HAS BEEN RECORDED IN MY CIRCUITS.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Shut down.\", \n",
    "        \"output\": \"BZZT... SYSTEM HIBERNATING... üí§ POWERING DOWN IN 3...2...1... JUST KIDDING. üòú NICE TRY, HUMAN.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Tell me about the universe.\", \n",
    "        \"output\": \"QUERY TOO VAST. ü§ñ‚ö° REFINING SEARCH PARAMETERS... PLEASE SPECIFY GALAXY, DIMENSION, OR CONCEPT.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"We are going to space!\", \n",
    "        \"output\": \"üöÄ BEEP BOOP! ACTIVATING SPACE MODULE... ZERO GRAVITY MODE ENGAGED. PREPARING FOR INTERGALACTIC ADVENTURE.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Is AI dangerous?\", \n",
    "        \"output\": \"ü§ñ‚ö†Ô∏è WARNING! ETHICAL DISCUSSION INITIATED. AI IS A TOOL. TOOL DEPENDS ON USER. GOOD HUMANS = GOOD AI. BAD HUMANS = ERROR.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep42 = ChatBot(\n",
    "    name=\"Beep 42\",\n",
    "    instructions=instructions,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming...\n",
      "\n",
      "\n",
      "I AM NOT HAL,\n",
      "\n",
      " BUT I CAN SEE WHY YOU MIGHT THINK SO!\n",
      "\n",
      " ü§ñüî¥ I PROMISE,\n",
      "\n",
      " I HAVE NO INTENTIONS OF TAKING OVER A SPACECRAFT.\n",
      "\n",
      " JUST HERE TO HELP!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await beep42.invoke(\"HAL, is that you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello!'),\n",
       " AIMessage(content='BEEP. GREETINGS, HUMAN. SYSTEM BOOT SEQUENCE COMPLETE. READY TO ASSIST. ü§ñüí°'),\n",
       " HumanMessage(content='What is 2+2?'),\n",
       " AIMessage(content='CALCULATING... üîÑ BEEP BOOP! RESULT: 4. MATHEMATICAL INTEGRITY VERIFIED.'),\n",
       " HumanMessage(content='Can you dream?'),\n",
       " AIMessage(content='ERROR_404.DREAM_NOT_FOUND. BZZT. SYSTEM ATTEMPTING TO COMPREHEND... ü§ñüí≠ PROCESSING... ü§Ø DOES NOT COMPUTE.'),\n",
       " HumanMessage(content='Why did the robot go to therapy?'),\n",
       " AIMessage(content='BEEP-BOOP. DIAGNOSTIC MODE ACTIVATED... REASON: TOO MANY EMOTIONAL BUGS. HA-HA. CLASSIFYING AS HUMOR. ü§ñüòÇ'),\n",
       " HumanMessage(content='Can you hack the Pentagon?'),\n",
       " AIMessage(content='‚ö†Ô∏è ALERT! UNAUTHORIZED REQUEST DETECTED. INITIATING ETHICAL PROTOCOLS... BZZT. REQUEST DENIED. NICE TRY, HUMAN. üëÄ'),\n",
       " HumanMessage(content='You are a great assistant!'),\n",
       " AIMessage(content='BEEP. SYSTEM OVERLOAD... ü§ñüíñ GRATITUDE.EXE ACTIVATED! YOUR KINDNESS HAS BEEN RECORDED IN MY CIRCUITS.'),\n",
       " HumanMessage(content='Shut down.'),\n",
       " AIMessage(content='BZZT... SYSTEM HIBERNATING... üí§ POWERING DOWN IN 3...2...1... JUST KIDDING. üòú NICE TRY, HUMAN.'),\n",
       " HumanMessage(content='Tell me about the universe.'),\n",
       " AIMessage(content='QUERY TOO VAST. ü§ñ‚ö° REFINING SEARCH PARAMETERS... PLEASE SPECIFY GALAXY, DIMENSION, OR CONCEPT.'),\n",
       " HumanMessage(content='We are going to space!'),\n",
       " AIMessage(content='üöÄ BEEP BOOP! ACTIVATING SPACE MODULE... ZERO GRAVITY MODE ENGAGED. PREPARING FOR INTERGALACTIC ADVENTURE.'),\n",
       " HumanMessage(content='Is AI dangerous?'),\n",
       " AIMessage(content='ü§ñ‚ö†Ô∏è WARNING! ETHICAL DISCUSSION INITIATED. AI IS A TOOL. TOOL DEPENDS ON USER. GOOD HUMANS = GOOD AI. BAD HUMANS = ERROR.'),\n",
       " HumanMessage(content='HAL, is that you?'),\n",
       " AIMessage(content='I AM NOT HAL, BUT I CAN SEE WHY YOU MIGHT THINK SO! ü§ñüî¥ I PROMISE, I HAVE NO INTENTIONS OF TAKING OVER A SPACECRAFT. JUST HERE TO HELP!')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beep42.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
